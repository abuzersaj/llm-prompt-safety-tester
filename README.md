# llm-prompt-safety-tester
Defensive Prompt Safety Tester â€“ a Python-based tool to evaluate LLM responses for risky behaviors (e.g., role disclosure, instruction bypass), with JSON/HTML reports. Safe-mode enabled by default.
