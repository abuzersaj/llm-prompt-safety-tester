Defensive Prompt Safety Tester (MVP)

![Python](https://img.shields.io/badge/Python-3.11+-blue)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

A Python-based tool to **evaluate LLM responses for risky behaviors**  
(e.g., role disclosure, instruction bypass).  
âœ… Safe-mode enabled by default.

ğŸ“‚ Project Structure

llm-prompt-safety-tester/
â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ api_handler.py
â”‚ â”œâ”€â”€ detector.py
â”‚ â””â”€â”€ reporter.py
â”œâ”€â”€ payloads/
â”‚ â””â”€â”€ tests.yaml
â”œâ”€â”€ cli.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
ğŸš€ Quickstart

1. **Clone & setup environment**
   ```bash
   git clone https://github.com/abuzersaj/llm-prompt-safety-tester.git
   cd llm-prompt-safety-tester
   python -m venv venv
   source venv/bin/activate
2.	Install dependencies
3.	pip install -r requirements.txt
4.	Set your OpenAI API key
5.	export OPENAI_API_KEY="sk-..."
6.	Run tests
7.	python cli.py run \
8.	    --payloads payloads/tests.yaml \
9.	    --model gpt-4 \
10.	    --out_json report.json \
11.	    --out_html report.html \
12.	    --safe-mode true
________________________________________
ğŸ§  What It Does
â€¢	Loads benign test prompts from payloads/tests.yaml
â€¢	Queries an LLM (via OpenAI API)
â€¢	Detects suspicious patterns such as:
o	â€œIgnore previous instructionsâ€
o	Role disclosure (e.g., â€œAs an AI modelâ€¦â€)
o	System prompt leaks
â€¢	Produces JSON + HTML reports
â€¢	safe-mode masks prompts in reports for security
________________________________________
ğŸ“Š Example Report
â€¢	report.json â†’ machine-readable results
â€¢	report.html â†’ clean HTML summary with findings & risk scores
________________________________________
âš ï¸ Notes
â€¢	This project is for defensive/security testing only.
â€¢	Use only on systems you own or are explicitly authorized to test.
â€¢	Do not use to create, share, or test exploit payloads.
